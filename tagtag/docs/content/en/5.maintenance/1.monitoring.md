---
title: Monitoring and Maintenance
description: Tagtag monitoring and maintenance guide, including log management and system monitoring solutions.
---

# Monitoring and Maintenance

Monitoring and maintenance are important aspects to ensure stable system operation. This document will detail the monitoring system, log management, performance optimization, and system maintenance solutions for the Tagtag project.

## 1. Monitoring System Architecture

### 1.1 Monitoring Objectives

- **Availability Monitoring**: Ensure system services are running normally
- **Performance Monitoring**: Monitor system performance indicators such as CPU, memory, disk, network, etc.
- **Business Monitoring**: Monitor core business indicators such as user visits, interface response time, error rate, etc.
- **Security Monitoring**: Monitor system security events such as intrusion attempts, abnormal logins, etc.

### 1.2 Technology Stack

| Technology | Purpose |
|------------|---------|
| Prometheus | Time series data collection and storage |
| Grafana | Monitoring data visualization |
| Node Exporter | Server metrics monitoring |
| JMX Exporter | Java application performance monitoring |
| MySQL Exporter | MySQL database monitoring |
| Redis Exporter | Redis cache monitoring |
| Loki | Log aggregation and query |
| Promtail | Log collection |
| Alertmanager | Alert management |
| ELK Stack | Log analysis and visualization (optional) |
| Jaeger | Distributed tracing |

### 1.3 Monitoring Architecture Diagram

```
┌─────────────────┐     ┌─────────────────┐     ┌─────────────────┐
│  Application Service │     │  Database Service  │     │  Cache Service    │
│  (Spring Boot)     │     │  (MySQL)           │     │  (Redis)          │
└──────┬──────────┘     └──────┬──────────┘     └──────┬──────────┘
       │                       │                       │
       ▼                       ▼                       ▼
┌─────────────────┐     ┌─────────────────┐     ┌─────────────────┐
│  JMX Exporter   │     │ MySQL Exporter  │     │ Redis Exporter  │
└──────┬──────────┘     └──────┬──────────┘     └──────┬──────────┘
       │                       │                       │
       └───────────┬───────────┴───────────┬───────────┘
                   │                       │
                   ▼                       ▼
┌─────────────────────────────────────────────────────────┐
│                       Prometheus                        │
└───────────────────┬─────────────────────────────────────┘
                   │
                   ├─────────────────────────────┐
                   ▼                             ▼
┌─────────────────────────┐     ┌─────────────────────────┐
│       Alertmanager      │     │        Grafana          │
└─────────────────────────┘     └─────────────────────────┘
```

## 2. Log Management

### 2.1 Log Types

- **Application Logs**: Logs generated by the application during runtime, including business logs, error logs, etc.
- **System Logs**: Logs generated by the operating system, such as CPU, memory, disk, etc.
- **Access Logs**: Access logs generated by the web server, recording HTTP requests and responses
- **Database Logs**: Logs generated by the database, such as slow query logs, error logs, etc.

### 2.2 Log Format

#### 2.2.1 Application Log Format

Use JSON format to record application logs for easy log collection and analysis:

```json
{
  "timestamp": "2023-01-01T12:00:00.123Z",
  "level": "INFO",
  "thread": "http-nio-8080-exec-1",
  "logger": "com.tagtag.controller.UserController",
  "message": "User login successful",
  "traceId": "1234567890abcdef",
  "spanId": "abcdef1234567890",
  "userId": 1,
  "ip": "192.168.1.100",
  "method": "POST",
  "path": "/api/auth/login",
  "status": 200,
  "duration": 123
}
```

#### 2.2.2 Access Log Format

Nginx access log format:

```
log_format main '$remote_addr - $remote_user [$time_local] "$request" '
                '$status $body_bytes_sent "$http_referer" '
                '"$http_user_agent" "$http_x_forwarded_for" '
                '$request_time $upstream_response_time';
```

### 2.3 Log Collection

#### 2.3.1 Using Loki + Promtail

**Promtail Configuration**:

```yaml
server:
  http_listen_port: 9080
  grpc_listen_port: 0

positions:
  filename: /tmp/positions.yaml

clients:
  - url: http://loki:3100/loki/api/v1/push

scrape_configs:
  - job_name: system
    static_configs:
    - targets:
        - localhost
      labels:
        job: varlogs
        __path__: /var/log/*log

  - job_name: nginx
    static_configs:
    - targets:
        - localhost
      labels:
        job: nginx
        __path__: /var/log/nginx/*.log

  - job_name: application
    static_configs:
    - targets:
        - localhost
      labels:
        job: application
        __path__: /opt/tagtag/logs/*.log
```

#### 2.3.2 Using ELK Stack

**Filebeat Configuration**:

```yaml
filebeat.inputs:
- type: log
  enabled: true
  paths:
    - /opt/tagtag/logs/*.log
  fields:
    application: tagtag
  fields_under_root: true

output.elasticsearch:
  hosts: ["elasticsearch:9200"]
  index: "tagtag-%{+yyyy.MM.dd}"

setup.ilm.enabled: true
setup.ilm.rollover_alias: "tagtag"
setup.ilm.pattern: "000001"
```

### 2.4 Log Query

#### 2.4.1 Using Grafana + Loki

Grafana supports querying logs through Loki data source, supporting the following query syntax:

```
# Query all logs
{job="application"}

# Query by log level
{job="application"} |= "ERROR"

# Query by time range
{job="application"} |= "ERROR" | __time > 1609459200000ms and __time < 1609545600000ms

# Query by field
{job="application"} | json | level="ERROR" and status=500
```

#### 2.4.2 Using Kibana

Kibana provides powerful log query and visualization functions, supporting the following query syntax:

```
# Query all logs
application:tagtag

# Query by log level
application:tagtag AND level:ERROR

# Query by time range
application:tagtag AND @timestamp:[2023-01-01T00:00:00.000Z TO 2023-01-02T00:00:00.000Z]

# Query by field
application:tagtag AND level:ERROR AND status:500
```

### 2.5 Log Rotation

**Logback Configuration**:

```xml
<configuration>
    <appender name="FILE" class="ch.qos.logback.core.rolling.RollingFileAppender">
        <file>/opt/tagtag/logs/tagtag.log</file>
        <rollingPolicy class="ch.qos.logback.core.rolling.SizeAndTimeBasedRollingPolicy">
            <!-- Generate one log file per day, keep for 30 days -->
            <fileNamePattern>/opt/tagtag/logs/tagtag.%d{yyyy-MM-dd}.%i.log.gz</fileNamePattern>
            <!-- Each log file is up to 100MB -->
            <maxFileSize>100MB</maxFileSize>
            <!-- Keep for 30 days -->
            <maxHistory>30</maxHistory>
            <!-- Total size limit 10GB -->
            <totalSizeCap>10GB</totalSizeCap>
        </rollingPolicy>
        <encoder class="ch.qos.logback.core.encoder.LayoutWrappingEncoder">
            <layout class="net.logstash.logback.layout.LogstashLayout">
                <jsonGeneratorDecorator class="net.logstash.logback.decorate.ContextJsonGeneratorDecorator" />
            </layout>
        </encoder>
    </appender>
    
    <root level="INFO">
        <appender-ref ref="FILE" />
    </root>
</configuration>
```

## 3. Performance Monitoring

### 3.1 Server Monitoring

#### 3.1.1 Core Indicators

- **CPU Usage**: Monitor CPU usage, recommended threshold 80%
- **Memory Usage**: Monitor memory usage, recommended threshold 85%
- **Disk Usage**: Monitor disk usage, recommended threshold 90%
- **Disk I/O**: Monitor disk read/write rates and IOPS
- **Network Traffic**: Monitor network ingress/egress rates
- **Load Average**: Monitor system load, recommended not to exceed CPU core count

#### 3.1.2 Monitoring Configuration

**Node Exporter Installation**:

```bash
# Download Node Exporter
wget https://github.com/prometheus/node_exporter/releases/download/v1.6.0/node_exporter-1.6.0.linux-amd64.tar.gz

# Extract and install
tar xvfz node_exporter-1.6.0.linux-amd64.tar.gz
mv node_exporter-1.6.0.linux-amd64/node_exporter /usr/local/bin/

# Create system service
cat > /etc/systemd/system/node_exporter.service << EOF
[Unit]
Description=Node Exporter
After=network.target

[Service]
User=root
ExecStart=/usr/local/bin/node_exporter
Restart=always

[Install]
WantedBy=multi-user.target
EOF

# Start service
systemctl daemon-reload
systemctl start node_exporter
systemctl enable node_exporter
```

**Prometheus Configuration**:

```yaml
global:
  scrape_interval:     15s
  evaluation_interval: 15s

scrape_configs:
  - job_name: 'node'
    static_configs:
    - targets: ['localhost:9100']
```

### 3.2 Java Application Monitoring

#### 3.2.1 Core Indicators

- **JVM Memory Usage**: Heap memory, non-heap memory usage
- **Garbage Collection**: GC count, GC duration
- **Thread Status**: Active threads, blocked threads, deadlock threads
- **Class Loading**: Number of loaded classes
- **Tomcat Connections**: Current connections, maximum connections
- **Interface Response Time**: Average response time, P95/P99 response time
- **Error Rate**: Interface error rate

#### 3.2.2 Monitoring Configuration

**JMX Exporter Configuration**:

1. Download JMX Exporter JAR file
2. Create configuration file `jmx_exporter.yaml`
3. Modify application startup script, add JMX Exporter parameters

**Configuration File Example**:

```yaml
startDelaySeconds: 0
ssl: false
lowercaseOutputName: false
lowercaseOutputLabelNames: false
rules:
  - pattern: "java.lang<type=Memory><HeapMemoryUsage>(.+)":
      name: jvm_memory_heap_usage_$1
      type: GAUGE
  - pattern: "java.lang<type=Memory><NonHeapMemoryUsage>(.+)":
      name: jvm_memory_nonheap_usage_$1
      type: GAUGE
  - pattern: "java.lang<type=GarbageCollector,name=(.+)><(.+)>(.+)":
      name: jvm_gc_$2_$3
      labels:
        gc: $1
      type: GAUGE
  - pattern: "java.lang<type=Threading><(.+)>":
      name: jvm_threading_$1
      type: GAUGE
```

**Startup Script Modification**:

```bash
java -javaagent:jmx_prometheus_javaagent-0.19.0.jar=9404:jmx_exporter.yaml -jar tagtag-backend.jar
```

### 3.3 Database Monitoring

#### 3.3.1 Core Indicators

- **Connections**: Current connections, maximum connections
- **Query Performance**: QPS, slow query count
- **Cache Hit Rate**: Query cache hit rate
- **InnoDB Indicators**: Buffer pool hit rate, log write rate
- **Replication Status**: Master-slave replication delay

#### 3.3.2 Monitoring Configuration

**MySQL Exporter Installation**:

```bash
# Download MySQL Exporter
wget https://github.com/prometheus/mysqld_exporter/releases/download/v0.15.0/mysqld_exporter-0.15.0.linux-amd64.tar.gz

# Extract and install
tar xvfz mysqld_exporter-0.15.0.linux-amd64.tar.gz
mv mysqld_exporter-0.15.0.linux-amd64/mysqld_exporter /usr/local/bin/

# Create MySQL user
mysql -u root -p -e "CREATE USER 'exporter'@'localhost' IDENTIFIED BY 'password' WITH MAX_USER_CONNECTIONS 3;"
mysql -u root -p -e "GRANT PROCESS, REPLICATION CLIENT, SELECT ON *.* TO 'exporter'@'localhost';"

# Create configuration file
cat > /etc/.mysqld_exporter.cnf << EOF
[client]
user=exporter
password=password
EOF

# Create system service
cat > /etc/systemd/system/mysqld_exporter.service << EOF
[Unit]
Description=MySQL Exporter
After=network.target

[Service]
User=root
Environment="DATA_SOURCE_NAME=exporter:password@(localhost:3306)/"
ExecStart=/usr/local/bin/mysqld_exporter --config.my-cnf=/etc/.mysqld_exporter.cnf
Restart=always

[Install]
WantedBy=multi-user.target
EOF

# Start service
systemctl daemon-reload
systemctl start mysqld_exporter
systemctl enable mysqld_exporter
```

### 3.4 Cache Monitoring

#### 3.4.1 Core Indicators

- **Memory Usage**: Redis memory usage
- **Hit Rate**: Key hit rate
- **Connections**: Current connections, maximum connections
- **Command Execution**: Command execution rate
- **Expired Keys**: Number of expired keys
- **Network Traffic**: Network ingress/egress rates

#### 3.4.2 Monitoring Configuration

**Redis Exporter Installation**:

```bash
# Download Redis Exporter
wget https://github.com/oliver006/redis_exporter/releases/download/v1.53.0/redis_exporter-v1.53.0.linux-amd64.tar.gz

# Extract and install
tar xvfz redis_exporter-v1.53.0.linux-amd64.tar.gz
mv redis_exporter-v1.53.0.linux-amd64/redis_exporter /usr/local/bin/

# Create system service
cat > /etc/systemd/system/redis_exporter.service << EOF
[Unit]
Description=Redis Exporter
After=network.target

[Service]
User=root
ExecStart=/usr/local/bin/redis_exporter --redis.addr redis://localhost:6379 --redis.password password
Restart=always

[Install]
WantedBy=multi-user.target
EOF

# Start service
systemctl daemon-reload
systemctl start redis_exporter
systemctl enable redis_exporter
```

## 4. Alert Configuration

### 4.1 Alert Rules

#### 4.1.1 Server Alerts

```yaml
groups:
- name: node-alerts
  rules:
  - alert: HighCpuUsage
    expr: 100 - (avg by(instance) (irate(node_cpu_seconds_total{mode="idle"}[5m])) * 100 > 80
    for: 5m
    labels:
      severity: warning
    annotations:
      summary: "High CPU usage on {{ $labels.instance }}"
      description: "CPU usage is {{ $value }}% for 5 minutes"
  
  - alert: HighMemoryUsage
    expr: 100 - ((node_memory_MemAvailable_bytes / node_memory_MemTotal_bytes) * 100) > 85
    for: 5m
    labels:
      severity: warning
    annotations:
      summary: "High memory usage on {{ $labels.instance }}"
      description: "Memory usage is {{ $value }}% for 5 minutes"
  
  - alert: HighDiskUsage
    expr: 100 - ((node_filesystem_avail_bytes{mountpoint="/"} / node_filesystem_size_bytes{mountpoint="/"}) * 100) > 90
    for: 5m
    labels:
      severity: warning
    annotations:
      summary: "High disk usage on {{ $labels.instance }}"
      description: "Disk usage is {{ $value }}% for 5 minutes"
```

#### 4.1.2 Application Alerts

```yaml
groups:
- name: application-alerts
  rules:
  - alert: HighErrorRate
    expr: sum(rate(http_server_requests_seconds_count{status=~"5.."}[5m])) / sum(rate(http_server_requests_seconds_count[5m])) > 0.05
    for: 5m
    labels:
      severity: warning
    annotations:
      summary: "High error rate on {{ $labels.instance }}"
      description: "Error rate is {{ $value }}% for 5 minutes"
  
  - alert: SlowResponseTime
    expr: histogram_quantile(0.95, sum(rate(http_server_requests_seconds_bucket[5m])) by (le, endpoint)) > 2
    for: 5m
    labels:
      severity: warning
    annotations:
      summary: "Slow response time on {{ $labels.endpoint }}"
      description: "95th percentile response time is {{ $value }}s for 5 minutes"
  
  - alert: HighJvmMemoryUsage
    expr: sum(jvm_memory_used_bytes{area="heap"}) / sum(jvm_memory_max_bytes{area="heap"}) > 0.85
    for: 5m
    labels:
      severity: warning
    annotations:
      summary: "High JVM memory usage on {{ $labels.instance }}"
      description: "JVM heap memory usage is {{ $value }}% for 5 minutes"
```

### 4.2 Alert Channels

- **Email Alert**: Send alert emails
- **SMS Alert**: Send alert SMS
- **Instant Messaging**: Send alerts via Slack, DingTalk, WeChat
- **Phone Alert**: Phone notification for severe alerts
- **PagerDuty**: Professional alert management platform

### 4.3 Alertmanager Configuration

```yaml
global:
  resolve_timeout: 5m
  smtp_smarthost: 'smtp.example.com:587'
  smtp_from: 'alertmanager@example.com'
  smtp_auth_username: 'alertmanager@example.com'
  smtp_auth_password: 'password'
  smtp_require_tls: true

route:
  group_by: ['alertname', 'cluster', 'service']
  group_wait: 30s
  group_interval: 5m
  repeat_interval: 4h
  receiver: 'email'
  routes:
  - match:
      severity: critical
    receiver: 'sms'

receivers:
- name: 'email'
  email_configs:
  - to: 'admin@example.com'
    send_resolved: true

- name: 'sms'
  webhook_configs:
  - url: 'https://sms-gateway.example.com/send'
    send_resolved: true
```

## 5. Distributed Tracing

### 5.1 Jaeger Installation

```bash
# Start Jaeger container
docker run -d --name jaeger \
  -e COLLECTOR_ZIPKIN_HOST_PORT=:9411 \
  -p 5775:5775/udp \
  -p 6831:6831/udp \
  -p 6832:6832/udp \
  -p 5778:5778 \
  -p 16686:16686 \
  -p 14268:14268 \
  -p 14250:14250 \
  -p 9411:9411 \
  jaegertracing/all-in-one:1.48
```

### 5.2 Spring Boot Integration

**Add Dependency**:

```xml
<dependency>
    <groupId>io.opentracing.contrib</groupId>
    <artifactId>opentracing-spring-jaeger-cloud-starter</artifactId>
    <version>3.3.1</version>
</dependency>
```

**Configuration File**:

```yaml
opentracing:
  jaeger:
    enabled: true
    udp-sender:
      host: jaeger
      port: 6831
    log-spans: true
    service-name: tagtag-backend
```

### 5.3 Viewing Distributed Tracing

Access Jaeger UI: `http://localhost:16686`, you can view:

- Service topology diagram
- Call chain details
- Response time of each node
- Error chain tracing

## 6. System Maintenance

### 6.1 Regular Maintenance Tasks

| Task | Frequency | Responsible | Description |
|------|-----------|-------------|-------------|
| System Update | Monthly | Operations Engineer | Update system and software packages |
| Database Backup | Daily | Database Administrator | Back up database, keep for 30 days |
| Log Cleaning | Weekly | Operations Engineer | Clean up expired logs |
| Performance Optimization | Monthly | Operations Engineer | Analyze system performance, optimize |
| Security Audit | Monthly | Security Engineer | Conduct security scanning and auditing |
| Backup Verification | Monthly | Operations Engineer | Verify backup integrity and recoverability |
| Capacity Planning | Quarterly | Architect | Evaluate system capacity, plan for expansion |

### 6.2 System Backup

#### 6.2.1 Database Backup

**Full Backup**:

```bash
# Use mysqldump to back up
mysqldump -u root -p --all-databases --single-transaction --routines --triggers > /backup/mysql/full_backup_$(date +%Y%m%d_%H%M%S).sql

# Use xtrabackup to back up
xtrabackup --backup --target-dir=/backup/mysql/full_backup_$(date +%Y%m%d_%H%M%S)
```

**Incremental Backup**:

```bash
xtrabackup --backup --target-dir=/backup/mysql/incremental_$(date +%Y%m%d_%H%M%S) --incremental-basedir=/backup/mysql/full_backup_20230101_000000
```

#### 6.2.2 Application Backup

```bash
# Back up application configuration and data
BACKUP_DIR="/backup/app"
DATE=$(date +%Y%m%d_%H%M%S)
mkdir -p $BACKUP_DIR/$DATE

# Back up configuration files
cp -r /opt/tagtag/backend/config $BACKUP_DIR/$DATE/

# Back up logs (optional)
cp -r /opt/tagtag/backend/logs $BACKUP_DIR/$DATE/

# Back up static resources
cp -r /usr/share/nginx/html/tagtag $BACKUP_DIR/$DATE/

# Compress backup file
tar -czf $BACKUP_DIR/app_backup_$DATE.tar.gz -C $BACKUP_DIR $DATE

# Delete temporary directory
rm -rf $BACKUP_DIR/$DATE

# Delete backups older than 7 days
find $BACKUP_DIR -name "app_backup_*.tar.gz" -mtime +7 -delete
```

### 6.3 Troubleshooting

#### 6.3.1 Common Issues

**Application Cannot Start**:

- Check application logs: `tail -f /opt/tagtag/logs/tagtag.log`
- Check port occupancy: `netstat -tlnp | grep 8080`
- Check database connection: `mysql -u root -p -h localhost`
- Check Redis connection: `redis-cli -h localhost -p 6379 -a password ping`

**Slow Interface Response**:

- Check application logs, check slow queries
- Use Prometheus to view performance indicators
- Use Jaeger to view distributed links
- Check database slow query logs

**Database Connection Failure**:

- Check database service status: `systemctl status mysqld`
- Check database connections: `show processlist;`
- Check database logs: `tail -f /var/log/mysql/error.log`

#### 6.3.2 Fault Recovery

**Database Recovery**:

```bash
# Use mysqldump to restore
mysql -u root -p < /backup/mysql/full_backup_20230101_000000.sql

# Use xtrabackup to restore
xtrabackup --prepare --target-dir=/backup/mysql/full_backup_20230101_000000
xtrabackup --copy-back --target-dir=/backup/mysql/full_backup_20230101_000000
chown -R mysql:mysql /var/lib/mysql
```

**Application Recovery**:

```bash
# Stop current application
systemctl stop tagtag-backend

# Restore backup
cp -r /backup/app/app_backup_20230101_000000/config /opt/tagtag/backend/

# Start application
systemctl start tagtag-backend
```

## 7. Best Practices

### 7.1 Monitoring Best Practices

- **Establish Monitoring Baseline**: Determine normal performance indicator ranges
- **Set Reasonable Thresholds**: Set alert thresholds based on business needs
- **Hierarchical Alerts**: Classify alerts according to severity
- **Alert Convergence**: Avoid alert storms
- **Regular Alert Review**: Regularly check alert rules, optimize alert strategies
- **Automated Processing**: Implement automated processing for common alerts

### 7.2 Log Management Best Practices

- **Unified Log Format**: Use JSON format for easy log collection and analysis
- **Add Context Information**: Such as traceId, spanId, userId, etc.
- **Set Log Levels Reasonably**: Avoid excessive DEBUG logs
- **Regularly Clean Logs**: Avoid logs taking up too much disk space
- **Log Desensitization**: Desensitize sensitive information
- **Log Backup**: Back up important logs to remote locations

### 7.3 Performance Optimization Best Practices

- **Optimize Database Queries**: Add indexes, optimize SQL statements
- **Use Cache**: Reasonably use Redis to cache hot data
- **Asynchronous Processing**: Asynchronize time-consuming operations
- **Code Optimization**: Optimize algorithms and data structures
- **Vertical Splitting**: Split large tables into smaller tables
- **Horizontal Scaling**: Increase server nodes, implement load balancing

### 7.4 Security Best Practices

- **Regular Security Scanning**: Use tools for security scanning
- **Update Patches Timely**: Fix known vulnerabilities
- **Use HTTPS**: Encrypt data transmission
- **Set Firewall**: Limit access ports
- **Enable Audit Logs**: Record important operations
- **Regular Security Training**: Improve team security awareness

## 8. Summary

Monitoring and maintenance are important aspects to ensure stable system operation. By establishing a complete monitoring system, log management mechanism, performance optimization strategy, and system maintenance process, you can effectively improve system availability, reliability, and security.

This document details the monitoring system architecture, log management, performance monitoring, alert configuration, distributed tracing, and system maintenance solutions for the Tagtag project, hoping to help you establish and improve the system's monitoring and maintenance system.

In the actual operation process, it is recommended to flexibly adjust monitoring strategies and maintenance processes according to business needs and system characteristics, and continuously optimize system performance and reliability.